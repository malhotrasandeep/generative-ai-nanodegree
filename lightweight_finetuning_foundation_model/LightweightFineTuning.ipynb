{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: GPT-2\n",
    "* Evaluation approach: Trainer\n",
    "* Fine-tuning dataset: sms-spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9484c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac13722b1800497899d0d566a11003d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afe1a988f254ea999ebe89e328c1286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee00026bf55a4d048eb0c8471ee23d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4294fa00c0f441be976d7bc0c759df0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sms', 'label'],\n",
      "    num_rows: 1115\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#load the sms_spam dataset for hugging face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sms_spam\",split=\"train\").train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=99)\n",
    "    \n",
    "splits = ['train','test']\n",
    "\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b605e6e8462412d980b1547b9df3c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3f1b5e3a0b4c5b8e3e8982305e2c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0eafaca1bb42c58be67f5261521d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef02a8da52a451487a3d97a9eaba5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0526edcf1f0c454483bfee368619f1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fa3ee7ea0240aebf9ca6c77b999f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf57c07c8324ea3aaafe41fa0699bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4459\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1115\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#use the GPT-2 tokinzer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#since the GPT2 tokenizer does not have pad_token, set it to eos_token\n",
    "#question: is this fine, or should we use a different token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(\n",
    "        lambda x:tokenizer(x[\"sms\"],truncation=True, padding=True),batched=True\n",
    "    )\n",
    "\n",
    "print(tokenized_ds['train'])\n",
    "print(tokenized_ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238e9e47060b45c68834086638fa6a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load the GPT2ForSequenceClassification model\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "#set the model padding token to same as tokenizer padding token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63af999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to compute metric\n",
    "#use accuracy metrics\n",
    "import numpy as np\n",
    "\n",
    "# metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return {\"accuracy\":(predictions==labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a trainer\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./data/temp\",\n",
    "            per_device_eval_batch_size=4,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\"),\n",
    "        eval_dataset=tokenized_ds['test'],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.291898727416992,\n",
       " 'eval_accuracy': 0.13632286995515694,\n",
       " 'eval_runtime': 13.9953,\n",
       " 'eval_samples_per_second': 79.67,\n",
       " 'eval_steps_per_second': 19.935}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the pre-trained model\n",
    "op = trainer.evaluate()\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69463d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pre-trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>9.291899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.136323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>13.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>79.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>19.935000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Pre-trained\n",
       "eval_loss                   9.291899\n",
       "eval_accuracy               0.136323\n",
       "eval_runtime               13.995300\n",
       "eval_samples_per_second    79.670000\n",
       "eval_steps_per_second      19.935000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pretrained = pd.DataFrame.from_dict(op, orient = 'index',columns=[\"Pre-trained\"])\n",
    "df_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PEFT Config\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Create a PEFT Model from the transformer model\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "from peft import get_peft_model\n",
    "\n",
    "model_1 = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "lora_model = get_peft_model(model_1, config)\n",
    "lora_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use hugging face trainer to train the mode\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "                model=lora_model,\n",
    "                args=TrainingArguments(\n",
    "                    output_dir='./data/peft',\n",
    "                    learning_rate=2e-3,\n",
    "                    per_device_train_batch_size=4,\n",
    "                    per_device_eval_batch_size=4,\n",
    "                    num_train_epochs=2,\n",
    "                    weight_decay=.01,\n",
    "                    evaluation_strategy='epoch',\n",
    "                    save_strategy='epoch',\n",
    "                    load_best_model_at_end=True\n",
    "                ),\n",
    "                train_dataset=tokenized_ds['train'],   \n",
    "                eval_dataset=tokenized_ds['test'],           \n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015d4736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2230' max='2230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2230/2230 06:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.140961</td>\n",
       "      <td>0.977578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.071448</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/peft/checkpoint-1115 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft/checkpoint-2230 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2230, training_loss=0.21804306560567677, metrics={'train_runtime': 416.0662, 'train_samples_per_second': 21.434, 'train_steps_per_second': 5.36, 'total_flos': 1082766751948800.0, 'train_loss': 0.21804306560567677, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/tokenizer_config.json',\n",
       " './model/tokenizer/special_tokens_map.json',\n",
       " './model/tokenizer/vocab.json',\n",
       " './model/tokenizer/merges.txt',\n",
       " './model/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model and the tokenizer\n",
    "lora_model.save_pretrained(\"./model/model\")\n",
    "tokenizer.save_pretrained(\"./model/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_saved_lora =  GPT2ForSequenceClassification.from_pretrained(\"./model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_saved_lora = GPT2Tokenizer.from_pretrained(\"./model/tokenizer\")\n",
    "model_saved_lora.config.pad_token_id = tokenizer_saved_lora.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer_2 = Trainer(\n",
    "                model=model_saved_lora,\n",
    "                args=TrainingArguments(\n",
    "                    output_dir='./data/peft-temp',\n",
    "                    per_device_eval_batch_size=4,\n",
    "                    evaluation_strategy='epoch',\n",
    "                    save_strategy='epoch'\n",
    "                ),\n",
    "                eval_dataset=tokenized_ds['test'],           \n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer_saved_lora,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "op = lora_trainer_2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39c7c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine Tuned (LoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.079422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.983857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>16.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>69.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>17.278000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Fine Tuned (LoRA)\n",
       "eval_loss                         0.079422\n",
       "eval_accuracy                     0.983857\n",
       "eval_runtime                     16.148000\n",
       "eval_samples_per_second          69.049000\n",
       "eval_steps_per_second            17.278000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_peft_lora = pd.DataFrame.from_dict(op, orient = 'index',columns=[\"Fine Tuned (LoRA)\"])\n",
    "df_peft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d87191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pre-trained</th>\n",
       "      <th>Fine Tuned (LoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>9.291899</td>\n",
       "      <td>0.079422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.136323</td>\n",
       "      <td>0.983857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>13.995300</td>\n",
       "      <td>16.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>79.670000</td>\n",
       "      <td>69.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>19.935000</td>\n",
       "      <td>17.278000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Pre-trained  Fine Tuned (LoRA)\n",
       "eval_loss                   9.291899           0.079422\n",
       "eval_accuracy               0.136323           0.983857\n",
       "eval_runtime               13.995300          16.148000\n",
       "eval_samples_per_second    79.670000          69.049000\n",
       "eval_steps_per_second      19.935000          17.278000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare evaluation of pre-trained and lora fine tuned\n",
    "df = df_pretrained.join(df_peft_lora)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c578611",
   "metadata": {},
   "source": [
    "# QLoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b3a826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#based on HuuginQuantization tutorial given at \n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "# import bitsandbytes\n",
    "\n",
    "# create the BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "#Pass the config the from_pretrained model\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "# model_ql = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", quantization_config=config)\n",
    "model_ql = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", quantization_config=config)\n",
    "model_ql.config.pad_token_id = tokenizer.pad_token_id\n",
    "#preprocess the quantized model for training\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model_ql = prepare_model_for_kbit_training(model_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f452e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#print the model to get the list of liner layers.\n",
    "#as \"all-linear\" module_type is not working\n",
    "print(model_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2354a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a LoRAConfig\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=32,\n",
    "    #     target_modules = [\"all-linear\"],       #QLoRA style training is giving error\n",
    "    target_modules=['c_attn', 'c_proj', 'c_fc'],   #so add layers manually\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "# use the get_peft_model() function to create a PeftModel from the quantized model and configuration\n",
    "\n",
    "from peft import get_peft_model\n",
    "model_qlora = get_peft_model(model_ql, config)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "                              output_dir=\"./model/qlora\",\n",
    "                              learning_rate=2e-3,\n",
    "                              per_device_train_batch_size=4,\n",
    "                              per_device_eval_batch_size=4,\n",
    "                              num_train_epochs=2,\n",
    "                              weight_decay=0.01,\n",
    "                              evaluation_strategy=\"epoch\",\n",
    "                              save_strategy=\"epoch\",\n",
    "                              load_best_model_at_end=True,\n",
    "                    )\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainer = Trainer(\n",
    "              model=model_qlora,\n",
    "              args=train_args,\n",
    "              train_dataset=tokenized_ds['train'],\n",
    "              eval_dataset=tokenized_ds['test'],\n",
    "              data_collator=DataCollatorWithPadding(tokenizer),\n",
    "              tokenizer=tokenizer,\n",
    "              compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb954fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2230' max='2230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2230/2230 11:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.074792</td>\n",
       "      <td>0.980269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.072296</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model/qlora/checkpoint-1115 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./model/qlora/checkpoint-2230 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2230, training_loss=0.14722054143657598, metrics={'train_runtime': 662.9061, 'train_samples_per_second': 13.453, 'train_steps_per_second': 3.364, 'total_flos': 1116558924410880.0, 'train_loss': 0.14722054143657598, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22981b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "op = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb147d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine Tuned (QLoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.072296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>20.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>53.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>13.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Fine Tuned (QLoRA)\n",
       "eval_loss                          0.072296\n",
       "eval_accuracy                      0.989238\n",
       "eval_runtime                      20.883600\n",
       "eval_samples_per_second           53.391000\n",
       "eval_steps_per_second             13.360000\n",
       "epoch                              2.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qlora = pd.DataFrame.from_dict(op, orient = 'index',columns=[\"Fine Tuned (QLoRA)\"])\n",
    "df_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a294797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pre-trained</th>\n",
       "      <th>Fine Tuned (LoRA)</th>\n",
       "      <th>Fine Tuned (QLoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>9.291899</td>\n",
       "      <td>0.079422</td>\n",
       "      <td>0.072296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.136323</td>\n",
       "      <td>0.983857</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>13.995300</td>\n",
       "      <td>16.148000</td>\n",
       "      <td>20.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>79.670000</td>\n",
       "      <td>69.049000</td>\n",
       "      <td>53.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>19.935000</td>\n",
       "      <td>17.278000</td>\n",
       "      <td>13.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Pre-trained  Fine Tuned (LoRA)  Fine Tuned (QLoRA)\n",
       "eval_loss                   9.291899           0.079422            0.072296\n",
       "eval_accuracy               0.136323           0.983857            0.989238\n",
       "eval_runtime               13.995300          16.148000           20.883600\n",
       "eval_samples_per_second    79.670000          69.049000           53.391000\n",
       "eval_steps_per_second      19.935000          17.278000           13.360000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare evaluation of all\n",
    "df = df.join(df_qlora)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63481c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
